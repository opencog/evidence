#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Aggregate Satisfaction
\end_layout

\begin_layout Date
17 April 2025
\end_layout

\begin_layout Author
Linas Vepštas
\end_layout

\begin_layout Abstract
Diary entry, attempting to work out some ideas.
 Wrttten in pseudo–chronological order.
 Stream–of–consciousness, kind of.
 Moving forward, I hope.
 Maybe going in circles.
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
What follows will be free-form unstructured writing about aggregate satisfiabili
ty.
 It is an attempt to reconcile a number of quite common and inter-related
 ideas, and to develop a mathematical frammework for them.
 The key ingredients are:
\end_layout

\begin_layout Itemize
The Ising model as a motivational example.
 This pulls in assorted concepts from physics that can be elaborated on.
\end_layout

\begin_layout Itemize
Link Grammar parsing, in which syntactic jigdaw pieces are assembled to
 create parses, called 
\begin_inset Quotes eld
\end_inset

linkages
\begin_inset Quotes erd
\end_inset

, that are assigned a score.
 The lower the linkage cost, the more likely correct linkage is correct.
\end_layout

\begin_layout Itemize
Aggregate satisfiability, in that satisfiability refers to discrete logical
 expressions, while aggregate suggests that there is some mean that indicates
 the best possible solution.
\end_layout

\begin_layout Standard
There are also some other inter-related concepts, which may of may not the
 ideas to be developed here.
 These include ideas like:
\end_layout

\begin_layout Itemize
Markov Logic Networks: perhaps what I intend to describe will end up being
 a Markov Logic Network.
 Or maybe not.
\end_layout

\begin_layout Itemize
Hopfield networks: these have commonality with regard to assorted physics
 concepts.
 Maybe they connect with what I intend to describe, and maybe they don't.
\end_layout

\begin_layout Itemize
Mean field theory.
 Who knows.
 A knee-jerk reaction is that mean–field theory will describe the rsults.
 I suppose.
 Seems plausible.
\end_layout

\begin_layout Itemize
Ensembles.
 Of course, everything to be considered is meant to be an ensemble.
 But I won't list this as a central point.
\end_layout

\begin_layout Standard
So here we go.
\end_layout

\begin_layout Section*
Ising Model
\end_layout

\begin_layout Standard
The part of the Ising Model that is interesting here is the Hamiltonian.
 It is a sum of pair–wise interactions of nearest neighbors:
\begin_inset Formula 
\[
\mathcal{H}=-\sum_{ij}J_{ij}\sigma_{i}\sigma_{j}-\mu\sum_{j}h_{j}\sigma_{j}
\]

\end_inset

with 
\begin_inset Formula $\sigma_{i}$
\end_inset

 the spins.
 I won't explain this.
 I copied it from Wikipedia.
 Read that article if you don't know what this is.
\end_layout

\begin_layout Section*
Link Grammar
\end_layout

\begin_layout Standard
I won't explain what Link Grammar (LG) is, or how it works.
 Read the docs.
 However, I want to write down a a Hamiltonian for it, which is something
 that the Link Grammar docs do not do.
 It is analogous to the Ising Model Hamiltonian:
\begin_inset Formula 
\[
\mathcal{H}=-\sum_{ijk\cdots}C_{ijk\cdots}w_{i}w_{j}w_{k}\cdots
\]

\end_inset

The 
\begin_inset Formula $w_{i}$
\end_inset

 are the words in the sentence.
 Written out here, in order to resemble the Ising Model.
 The 
\begin_inset Formula $C_{ijk\cdots}$
\end_inset

 is meant to be the LG cost, a single real–valued floating point number,
 of the the LG disjunct 
\begin_inset Formula $ijk\cdots$
\end_inset

.
\end_layout

\begin_layout Standard
The analogy here is strained, and the notation is inadequate.
 This is a brainstorm idea, so we'll fix the problems here later.
 If the idea pans out.
\end_layout

\begin_layout Standard
What are the 
\begin_inset Formula $w_{i}$
\end_inset

 really? One interpretation is to say 
\begin_inset Formula $w_{i}=+1$
\end_inset

 always, no matter what, and then the sum is simply a sum over the costs
 of the disjuncts appearing in the sentence.
 This is the conventional LG model.
 Another intrpretation is to write 
\begin_inset Formula $w_{i}=+1$
\end_inset

 if that word appears in position 
\begin_inset Formula $i$
\end_inset

 in the sentence, else it is zero.
 To arrive at this, we have to be more carefuly with the notation: Define
\begin_inset Formula 
\[
\delta\left(w,v\right)=\begin{cases}
1 & \mbox{if }w=v\\
0 & \mbox{if }w\ne v
\end{cases}
\]

\end_inset

where 
\begin_inset Formula $w,v$
\end_inset

 are specific words in the lexis.
 Then the Hamiltonian is
\begin_inset Formula 
\[
\mathcal{H}=-\sum_{uv\cdots}C_{uv\cdots}\delta\left(u,w_{i}\right)\delta\left(v,w_{j}\right)\delta\left(\cdots\right)\cdots
\]

\end_inset

so that the subscript 
\begin_inset Formula $i$
\end_inset

 encodes the position of a word in a sentence, and 
\begin_inset Formula $w_{i}$
\end_inset

 is the word in that position.
\end_layout

\begin_layout Standard
This interpretation is unsatisfying, and wrong, the analogy doesn't work.
 First of all, the spins in the Ising model can flip up and down, whereas
 the observed word-order in a given sentence is fixed, inviolable.
 The above picture could work for sentence generation, where we are considering
 how to generate some senttence, but that is not yet currently viable given
 what is written here, so far,
\end_layout

\begin_layout Standard
The other issue is that, in classical LG, the disjunct costs 
\begin_inset Formula $C_{uvw\cdots}$
\end_inset

 are hand-crafted by the academic scholar defining the lexis.
 The learn project attempts to replace these costs by those learned from
 a training corpus, starting from a pair-wise frequenist counting scheme.
 The 
\begin_inset Formula $C_{uvw\cdots}$
\end_inset

 are interpreted as mutual entropies, not energies.
 There is a reasonable argument for why it should be entropy.
 For the present case, we note that energies are also additive, so we can
 keep our minds open about what is being additive, here: energy, information,
 something else abelian, maybe.
\end_layout

\begin_layout Standard
The symbolic learning project is to determine the lexis automatically.
 To learn the correct numerical values of
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $C_{uvw\cdots}$
\end_inset

.
 And I want to do this by writing down some kind of Hamiltonian–like summation,
 and attempting to minimize that.
 But to do that, I need a dramatic change of notation.
 Lets try it.
 See what happens.
 I have no idea if this is a good idea or a bad idea.
 Or an old idea or a new idea.
\end_layout

\begin_layout Section*
Corpus Model
\end_layout

\begin_layout Standard
So, first, define a corpus: a collection of sentences.
 For the moment, let this be 
\begin_inset Quotes eld
\end_inset

bag of sentences
\begin_inset Quotes erd
\end_inset

, in jumbled order.
 A 
\begin_inset Quotes eld
\end_inset

sentence
\begin_inset Quotes erd
\end_inset

 double be a dozen words.
 But it could also be an ordered sequence of 100K words, i.e.
 a book, article, newspaper story, twitter/bluesky post, whatever.
 The length is immaterial; instead, we just take a collection of them, assumed
 to be independent.
 For now.
 Of course, ten books written on the same topic are not really independent
 of one-another, but lets not get ahead of ourselves.
\end_layout

\begin_layout Standard
The corpus is 
\begin_inset Formula $\mathcal{C}=\left\{ S_{a}\right\} $
\end_inset

 which is a set of sentences 
\begin_inset Formula $S_{a}$
\end_inset

 labelled with some index 
\begin_inset Formula $a$
\end_inset

.
 There is a total of 
\begin_inset Formula $\left|\mathcal{C}\right|$
\end_inset

 sentences, where 
\begin_inset Formula $\left|\cdot\right|$
\end_inset

 denotes the size (cardinal) of a set.
 The Hamiltonian is then
\begin_inset Formula 
\[
\mathcal{H}=\sum_{S_{a}\in\mathcal{C}}H\left(S_{a}\right)
\]

\end_inset

where the sum ranges over all sentences in the corpus, and 
\begin_inset Formula $H\left(S_{a}\right)$
\end_inset

 is the total cost of the parse (linkage) of that sentence.
\end_layout

\begin_layout Standard
And so here we meet our first conceptual problem: the linkage is not unique:
 any given sentence may have many linkages, and, in fact, we have to consdier,
 in principle, all possible linkages.
\end_layout

\begin_layout Standard
So here we flop over the a graph/network representation.
 First, lets drop the subscript 
\begin_inset Formula $a$
\end_inset

 on 
\begin_inset Formula $S_{a}$
\end_inset

 and just say 
\begin_inset Formula $S\in\mathcal{C}$
\end_inset

 is some sentence in the training corpus, and the Hamiltonian will be a
 sum over these.
 That doesn't change.
\end_layout

\begin_layout Standard
Next, we say that each sentence 
\begin_inset Formula $S=\left[w_{1},w_{2},\cdots,w_{n}\right]$
\end_inset

 is a temporal sequence of specific words 
\begin_inset Formula $w_{i}$
\end_inset

 aka 
\begin_inset Quotes eld
\end_inset

tokens
\begin_inset Quotes erd
\end_inset

 in conventional machine learning.
 Here, the index 
\begin_inset Formula $i$
\end_inset

 has a specific meaning: it indicates word–order.
 Sequential time.
\end_layout

\begin_layout Standard
For each sentence 
\begin_inset Formula $S$
\end_inset

, consider the set 
\begin_inset Formula $\mathcal{G}\left(S\right)=\left\{ G\left(S\right)\right\} $
\end_inset

 of all possible connected graphs 
\begin_inset Formula $G\left(S\right)$
\end_inset

 for that sentence.
 A graph here is a set of edges and vertexes: 
\begin_inset Formula $G=\left\{ V,E\right\} $
\end_inset

 (dropping the indicator 
\begin_inset Formula $S$
\end_inset

 because we fix the sentence for now.) For a given sentence 
\begin_inset Formula $S$
\end_inset

, the set of vertexes is fixed: 
\begin_inset Formula $V=\left\{ w_{i}|w_{i}\in S\right\} $
\end_inset

 is the set of the words in the sentence 
\begin_inset Formula $S$
\end_inset

.
 As a set, the order no longer really matters.
 But it is a multi-set, so its OK to have a word show repeatedly.
 An edge is just a conventional graph edge: it is an ordered pair of vertexes.
 Notation: 
\begin_inset Formula $E_{ij}=\left(w_{i},w_{j}\right)$
\end_inset

 is the edge connecting words at locations 
\begin_inset Formula $i,j$
\end_inset

.
 It is an ordered pair: edges have direction.
 For a fixed graph 
\begin_inset Formula $G$
\end_inset

 the set of edges can be thought of that the adjacency matrix.
 This is all very super-basic stuff, but I am trying to be as clear as posible
 here.
 Sorry.
 The adjacency matrix is usually take to be a matrix woth just ones and
 zeros in it, with one's taken to mean there is an edge, and zero meaning
 there is not.
 This can be generalized, but we're not doing that yet.
\end_layout

\begin_layout Standard
A connected graph is one where every vertex is reachable by a path from
 any other vertex.
 We want to consider only connected graphs; the ability to disconnect will
 be handled via a different machanism.
\end_layout

\begin_layout Standard
For a fixed senttence 
\begin_inset Formula $S$
\end_inset

, write the Hamiltonian
\begin_inset Formula 
\[
H\left(S\right)=\sum_{G\in\mathcal{G}}h\left(G\right)
\]

\end_inset

The sum ranges over all possible connected graphs over the words in the
 sentence.
\end_layout

\begin_layout Standard
Here, we arrive at the next stumbling block.
 Is it correct to call 
\begin_inset Formula $h\left(G\right)$
\end_inset

 the energy, or is it something else? The above has the classic form of
 a superposition.
 If we interpret a single graph to be a 
\begin_inset Quotes eld
\end_inset

state
\begin_inset Quotes erd
\end_inset

, then the above is a superposition of states, and 
\begin_inset Formula $H\left(S\right)$
\end_inset

 should be interpreted as the partition function, and not the energy.
 For partition functions, the appropriate notation would be a Boltzmann
 sum:
\begin_inset Formula 
\[
Z_{\beta}=\sum_{G\in\mathcal{G}}e^{-\beta h\left(G\right)}
\]

\end_inset

So, which is it? I'm confused, here.
 
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
That's all for now.
\end_layout

\end_body
\end_document
